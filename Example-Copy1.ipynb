{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using FAI to solve Atari environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLDR \n",
    "\n",
    "1. In the notebook toolbar click Kernel -> **Restart & Run all**.\n",
    "2. **Wait a bit**. Usually under 3 minutes in a laptop(1 i7 core) when you are lucky, 10 minutes when you are not.\n",
    "3. **You should have solved** the **Boxing-v0** Atari environment using a uniform prior and 75 samples per action.\n",
    "4. There is a **video** of the game played **inside** the ***videos* folder** of this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import everything we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fractalai.policy import GreedyPolicy\n",
    "from fractalai.model import RandomDiscreteModel\n",
    "from fractalai.fractalai import FractalAI\n",
    "from fractalai.environment import AtariEnvironment\n",
    "from fractalai.monitor import AtariMonitorPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table represents the highest scores that we have achieved in the games we tried. Please write us if you try to replicate them, specially if you don't succeed. \n",
    "\n",
    "You can use this as a quick cheat sheet to choose parameters that should work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Game</th>\n",
    "      <th>FAI Score</th>\n",
    "      <th>% vs Best AI</th>\n",
    "      <th>% vs MCTS</th>\n",
    "      <th>Mean samples / action</th>\n",
    "      <th>N repeat action</th>\n",
    "      <th>Time horizon</th>\n",
    "      <th>Max samples / action</th>\n",
    "      <th>Max states</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>alien</td>\n",
    "      <td>19,380</td>\n",
    "      <td>271.89%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>1,190</td>\n",
    "      <td>5</td>\n",
    "      <td>25</td>\n",
    "      <td>2,000</td>\n",
    "      <td>50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>amidar</td>\n",
    "      <td>4,306</td>\n",
    "      <td>194.40%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>1,222</td>\n",
    "      <td>5</td>\n",
    "      <td>25</td>\n",
    "      <td>2,000</td>\n",
    "      <td>50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>assault</td>\n",
    "      <td>1,280</td>\n",
    "      <td>17.06%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>1,317</td>\n",
    "      <td>5</td>\n",
    "      <td>25</td>\n",
    "      <td>2,000</td>\n",
    "      <td>50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>asteroids</td>\n",
    "      <td>76,270</td>\n",
    "      <td>160.94%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>2,733</td>\n",
    "      <td>2</td>\n",
    "      <td>20</td>\n",
    "      <td>5,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>beam rider</td>\n",
    "      <td>2,160</td>\n",
    "      <td>12.64%</td>\n",
    "      <td>29.86%</td>\n",
    "      <td>4,052</td>\n",
    "      <td>5</td>\n",
    "      <td>25</td>\n",
    "      <td>2,000</td>\n",
    "      <td>100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>boxing</td>\n",
    "      <td>100</td>\n",
    "      <td>104.17%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>2,027</td>\n",
    "      <td>3</td>\n",
    "      <td>30</td>\n",
    "      <td>2,000</td>\n",
    "      <td>150</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>breakout</td>\n",
    "      <td>36</td>\n",
    "      <td>7.98%</td>\n",
    "      <td>8.87%</td>\n",
    "      <td>5,309</td>\n",
    "      <td>5</td>\n",
    "      <td>30</td>\n",
    "      <td>20,000</td>\n",
    "      <td>150</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>centipede</td>\n",
    "      <td>529,355</td>\n",
    "      <td>4,405.05%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>1,960</td>\n",
    "      <td>1</td>\n",
    "      <td>20</td>\n",
    "      <td>6,000</td>\n",
    "      <td>100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>double dunk</td>\n",
    "      <td>20</td>\n",
    "      <td>400.00%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>5,327</td>\n",
    "      <td>3</td>\n",
    "      <td>50</td>\n",
    "      <td>8,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>enduro</td>\n",
    "      <td>471</td>\n",
    "      <td>28.05%</td>\n",
    "      <td>59.77%</td>\n",
    "      <td>nan</td>\n",
    "      <td>5</td>\n",
    "      <td>15</td>\n",
    "      <td>2,000</td>\n",
    "      <td>50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>ice hockey</td>\n",
    "      <td>52</td>\n",
    "      <td>5,200.00%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>12,158</td>\n",
    "      <td>3</td>\n",
    "      <td>50</td>\n",
    "      <td>20,000</td>\n",
    "      <td>250</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>ms pacman</td>\n",
    "      <td>58,521</td>\n",
    "      <td>372.91%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>5,129</td>\n",
    "      <td>10</td>\n",
    "      <td>20</td>\n",
    "      <td>20,000</td>\n",
    "      <td>250</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>qbert</td>\n",
    "      <td>35,750</td>\n",
    "      <td>189.66%</td>\n",
    "      <td>189.66%</td>\n",
    "      <td>2,728</td>\n",
    "      <td>5</td>\n",
    "      <td>20</td>\n",
    "      <td>5,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>seaquest</td>\n",
    "      <td>3,180</td>\n",
    "      <td>7.56%</td>\n",
    "      <td>97.64%</td>\n",
    "      <td>6,252</td>\n",
    "      <td>5</td>\n",
    "      <td>25</td>\n",
    "      <td>20,000</td>\n",
    "      <td>250</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>space invaders</td>\n",
    "      <td>3,605</td>\n",
    "      <td>80.29%</td>\n",
    "      <td>153.14%</td>\n",
    "      <td>4,261</td>\n",
    "      <td>2</td>\n",
    "      <td>35</td>\n",
    "      <td>6,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>15</th>\n",
    "      <td>tennis</td>\n",
    "      <td>16</td>\n",
    "      <td>177.78%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>2,437</td>\n",
    "      <td>4</td>\n",
    "      <td>30</td>\n",
    "      <td>5,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>16</th>\n",
    "      <td>video pinball</td>\n",
    "      <td>496,681</td>\n",
    "      <td>64.64%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>1,083</td>\n",
    "      <td>2</td>\n",
    "      <td>30</td>\n",
    "      <td>5,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>17</th>\n",
    "      <td>wizard of wor</td>\n",
    "      <td>93,090</td>\n",
    "      <td>785.44%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>2,229</td>\n",
    "      <td>4</td>\n",
    "      <td>35</td>\n",
    "      <td>5,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the parameter choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent relies on four parmeters:\n",
    "\n",
    "- **Fixed steps**: Although this parameter actually depends on the Environment, we can use it to manually set the frecuancy at which the Agent will play. Taking more consecutive actions allows for exploring further in the future at the cost of less reaction time.\n",
    "\n",
    "\n",
    "- **Time Horizon**: This value represents \"how far we need to look into the future when taking an action\". A useful rule of thumb is **Time Horiozon = Nt / Fixed steps**, where **Nt** is the number of frames that it takes the agent to loose one life, (die) since the moment it performs the actions that inevitably leads to its death. \n",
    "\n",
    "\n",
    "- **Max states**: This is the maximum number of states that can be part of the Swarm. This number is related to \"how thick\" we want the resulting causal cone to be. The algorithm will try to use the maximum number of states possible unless it detects it is wasting computation.\n",
    "\n",
    "\n",
    "- **Max samples**: This is the maximum number of times that we can sample an action when using a Swarm to build a causal cone. It is a superior bound, and the algorithm will try to use less samples to meet the defined **time horizon**. It is a nice way to limit how fast you need to take an action. A reasonable value could be **max walkers** \\* **time horizon** \\* ***N***, being ***N=5*** a number that works well in Atari games, but it depends on what you are trying to accomplish.\n",
    "\n",
    "\n",
    "- **Mean samples**: You cannot directly set this parameter. It is the mean number of samples taken each state. In and ideal case it would be **Max states \\* Time horizon** samples. If its lower it means that we are not having any trouble sampling the state space, but if its higher it means that the states tend to die and more computation has been be required. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Minimal Pacman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will tune the Agent to get a decent score on MsPacman using the minimum amount of computational resources possible. We will deliberately set a very small amount of computational resources for calculating an action.\n",
    "\n",
    "Doing that we want to address concerns about edge cases of the theory, by showing how the algorithm performs when the size of the swarm Swarm is very little with respect to the size of the state space.\n",
    "\n",
    "In order to do so, we can give the parameters the following values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "name = \"MsPacman-v0\"\n",
    "render=True # It is funnier if the game is displayed on the screen\n",
    "clone_seeds = True # This will speed things up a bit\n",
    "max_steps = 1e6 # Play until the game is finished.\n",
    "skip_frames = 80 # The Agent cannot do anything anyway, so its faster if we skip some frames at the begining\n",
    "n_fixed_steps=5 # Atari games run at 20 fps, so taking 4 actions per seconds is enough to finish the first level```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAI parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "max_samples=300 # Let see how well it can perform using 300 samples per step\n",
    "max_states=15 # Let's set a really small number to make everthing faster\n",
    "time_horizon=10 # 50 frames should be enough to realise you have been eaten by a ghost```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these parameters we are aiming for 150 samples per step, saving up to another 150 samples in case the agent runs into trouble. Using such a low number of samples will mean that the performance could vary widely among different runs, but in our tests, this agent was capable of finishing the first level most of the times.\n",
    "\n",
    "If you want to get better scores, just increase the values of the parameters accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available atari games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-cf616e3c9815>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-cf616e3c9815>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    <<< ['adventure',\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    ">>> import atari_py as ap\n",
    ">>> game_list = ap.list_games()\n",
    ">>> print(sorted(game_list))\n",
    "<<< ['adventure',\n",
    "     'air_raid',\n",
    "     'alien',\n",
    "     'amidar',\n",
    "     'assault',\n",
    "     'asterix',\n",
    "     'asteroids',\n",
    "     'atlantis',\n",
    "     'bank_heist',\n",
    "     'battle_zone',\n",
    "     'beam_rider',\n",
    "     'berzerk',\n",
    "     'bowling',\n",
    "     'boxing',\n",
    "     'breakout',\n",
    "     'carnival',\n",
    "     'centipede',\n",
    "     'chopper_command',\n",
    "     # ...\n",
    "     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Minimal Boxing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the same principles as in the former example, but this time aiming to solve the game Boxing. This probably turns out to be the least complex Atari game, and it can be solved with very little amount of computation involved.\n",
    "\n",
    "A funny thing about this game is the only one with the same reward scheme that is supposed to be solved in the minimum amount of time possible. But without stating it explicitely.\n",
    "\n",
    "A consequence of that, the agent sometimes just starts avoiding the opponent after achieving 99 points, because it considers a terminal state as a death, and it tries to keep himself alive until the time runs out.\n",
    "\n",
    "There are many ways of addressing this problem, but they are out of the scope of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"MsPacman-v0\"\n",
    "render=True # It is funnier if the game is displayed on the screen\n",
    "clone_seeds = True # This will speed things up a bit\n",
    "max_steps = 1e6 # Play until the game is finished.\n",
    "skip_frames = 80 # The Agent cannot do anything anyway, so its faster if we skip some frames at the begining\n",
    "n_fixed_steps=5 # Atari games run at 20 fps, so taking 4 actions per seconds is enough to finish the first level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAI parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples=300 # Let see how well it can perform using 300 samples per step\n",
    "max_states=15 # Let's set a really small number to make everthing faster\n",
    "time_horizon=10 # 50 frames should be enough to realise you have been eaten by a ghost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use a random prior\n",
    "env =  AtariEnvironment(name=name, clone_seeds=clone_seeds)\n",
    "model = RandomDiscreteModel(env.env.action_space.n)\n",
    "greedy = GreedyPolicy(env=env, model=model)\n",
    "\n",
    "fractal = FractalAI(policy=greedy, max_samples=max_samples, max_states=max_states,\n",
    "                    time_horizon=time_horizon, n_fixed_steps=n_fixed_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs faster if you are not recording, uncomment if you want to run a simulation.\n",
    "# fractal.evaluate(render=render, max_steps=max_steps, skip_frames=skip_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will make everything a bit slower, but you will be recording the game with an OpenAI monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"videos_monte\" # Folder where you can save the video.\n",
    "force = True # Override previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e19976f5bac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmonitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAtariMonitorPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfractal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/FractalAI/fractalai/monitor.py\u001b[0m in \u001b[0;36mrecord_video\u001b[0;34m(self, skip_frames)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mepisode_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskip_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmoni_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoni_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mmoni_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mepisode_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/policy.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \"\"\"\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/fractalai.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, state, n_fixed_steps)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreset_walkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_walkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fixed_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_i_simulation\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_time_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/fractalai.py\u001b[0m in \u001b[0;36m_step_states\u001b[0;34m(self, init_step, n_fixed_steps)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         self.states[step_ix] = self.step(self.states[step_ix], actions,\n\u001b[0;32m--> 153\u001b[0;31m                                          fixed_steps=n_fixed_steps)\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Save action taken from root state inside\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minit_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/policy.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, fixed_steps)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         return [self._step(state=si, action=ai, fixed_steps=fixed_steps)\n\u001b[0;32m--> 143\u001b[0;31m                 for si, ai in zip(state, action)]\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mskip_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/policy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         return [self._step(state=si, action=ai, fixed_steps=fixed_steps)\n\u001b[0;32m--> 143\u001b[0;31m                 for si, ai in zip(state, action)]\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mskip_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_frames\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/policy.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, state, action, fixed_steps)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/FractalAI/fractalai/policy.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, fixed_steps)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         return [self._step(state=si, action=ai, fixed_steps=fixed_steps)\n\u001b[1;32m    143\u001b[0m                 for si, ai in zip(state, action)]\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/policy.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, state, action, fixed_steps)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mProcessed\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mstate\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msimulation\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     def step(self, state: [State, Iterable], action: [int, np.ndarray, list, tuple],\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, fixed_steps)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \"\"\"\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/environment.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, state, action, fixed_steps)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \"\"\"\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_simulation_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfixed_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     def step(self, state: [State, Iterable], action: np.ndarray, fixed_steps: int=1) -> [State,\n",
      "\u001b[0;32m~/github/FractalAI/fractalai/environment.py\u001b[0m in \u001b[0;36mstep_simulation\u001b[0;34m(self, action, fixed_steps)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mmicrostate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloneState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         self.state.update_state(observed=observed, reward=self._cum_reward,\n\u001b[0m\u001b[1;32m    344\u001b[0m                                 end=end, lives=lives, microstate=Microstate(self.env, microstate))\n\u001b[1;32m    345\u001b[0m         \u001b[0;31m# self.state._dead = _dead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "monitor = AtariMonitorPolicy(policy=fractal, directory=directory, force=force)\n",
    "monitor.record_video(skip_frames=skip_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Check the video folder to watch what you recorded!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will really appreciate your feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor.monitor.env.close() # it should close when the game finishes, but just to be sure you can run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
