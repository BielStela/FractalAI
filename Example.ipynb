{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using FAI to solve Atari environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLDR \n",
    "\n",
    "1. In the notebook tooClick Kernel -> Restart & Run all.\n",
    "2. Wait a bit. Usually under 3 minutes in a laptop(1 i7 core) when you are lucky, 10 minutes when you are not.\n",
    "3. You just solved Boxing.\n",
    "4. There is a video of the game inside the video folder of this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import everything we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fractalai.policy import GreedyPolicy\n",
    "from fractalai.model import RandomDiscreteModel\n",
    "from fractalai.fractalai import FractalAI\n",
    "from fractalai.environment import AtariEnvironment\n",
    "from fractalai.monitor import AtariMonitorPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table represents the highest scores that we have achieved in the games we tried. Please write us if you try to replicate them, specially if you don't succeed. \n",
    "\n",
    "You can use this as a quick cheat sheet to choose parameters that should work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Game</th>\n",
    "      <th>FAI Score</th>\n",
    "      <th>% vs Best AI</th>\n",
    "      <th>% vs MCTS</th>\n",
    "      <th>Mean samples / action</th>\n",
    "      <th>N repeat action</th>\n",
    "      <th>Time horizon</th>\n",
    "      <th>Max samples / action</th>\n",
    "      <th>Max states</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>alien</td>\n",
    "      <td>19,380</td>\n",
    "      <td>271.89%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>1,190</td>\n",
    "      <td>5</td>\n",
    "      <td>25</td>\n",
    "      <td>2,000</td>\n",
    "      <td>50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>amidar</td>\n",
    "      <td>4,306</td>\n",
    "      <td>194.40%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>1,222</td>\n",
    "      <td>5</td>\n",
    "      <td>25</td>\n",
    "      <td>2,000</td>\n",
    "      <td>50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>assault</td>\n",
    "      <td>1,280</td>\n",
    "      <td>17.06%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>1,317</td>\n",
    "      <td>5</td>\n",
    "      <td>25</td>\n",
    "      <td>2,000</td>\n",
    "      <td>50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>asteroids</td>\n",
    "      <td>76,270</td>\n",
    "      <td>160.94%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>2,733</td>\n",
    "      <td>2</td>\n",
    "      <td>20</td>\n",
    "      <td>5,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>beam rider</td>\n",
    "      <td>2,160</td>\n",
    "      <td>12.64%</td>\n",
    "      <td>29.86%</td>\n",
    "      <td>4,052</td>\n",
    "      <td>5</td>\n",
    "      <td>25</td>\n",
    "      <td>2,000</td>\n",
    "      <td>100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>boxing</td>\n",
    "      <td>100</td>\n",
    "      <td>104.17%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>2,027</td>\n",
    "      <td>3</td>\n",
    "      <td>30</td>\n",
    "      <td>2,000</td>\n",
    "      <td>150</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>breakout</td>\n",
    "      <td>36</td>\n",
    "      <td>7.98%</td>\n",
    "      <td>8.87%</td>\n",
    "      <td>5,309</td>\n",
    "      <td>5</td>\n",
    "      <td>30</td>\n",
    "      <td>20,000</td>\n",
    "      <td>150</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>centipede</td>\n",
    "      <td>529,355</td>\n",
    "      <td>4,405.05%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>1,960</td>\n",
    "      <td>1</td>\n",
    "      <td>20</td>\n",
    "      <td>6,000</td>\n",
    "      <td>100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>double dunk</td>\n",
    "      <td>20</td>\n",
    "      <td>400.00%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>5,327</td>\n",
    "      <td>3</td>\n",
    "      <td>50</td>\n",
    "      <td>8,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>enduro</td>\n",
    "      <td>471</td>\n",
    "      <td>28.05%</td>\n",
    "      <td>59.77%</td>\n",
    "      <td>nan</td>\n",
    "      <td>5</td>\n",
    "      <td>15</td>\n",
    "      <td>2,000</td>\n",
    "      <td>50</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>ice hockey</td>\n",
    "      <td>52</td>\n",
    "      <td>5,200.00%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>12,158</td>\n",
    "      <td>3</td>\n",
    "      <td>50</td>\n",
    "      <td>20,000</td>\n",
    "      <td>250</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>ms pacman</td>\n",
    "      <td>58,521</td>\n",
    "      <td>372.91%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>5,129</td>\n",
    "      <td>10</td>\n",
    "      <td>20</td>\n",
    "      <td>20,000</td>\n",
    "      <td>250</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>qbert</td>\n",
    "      <td>35,750</td>\n",
    "      <td>189.66%</td>\n",
    "      <td>189.66%</td>\n",
    "      <td>2,728</td>\n",
    "      <td>5</td>\n",
    "      <td>20</td>\n",
    "      <td>5,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>seaquest</td>\n",
    "      <td>3,180</td>\n",
    "      <td>7.56%</td>\n",
    "      <td>97.64%</td>\n",
    "      <td>6,252</td>\n",
    "      <td>5</td>\n",
    "      <td>25</td>\n",
    "      <td>20,000</td>\n",
    "      <td>250</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>space invaders</td>\n",
    "      <td>3,605</td>\n",
    "      <td>80.29%</td>\n",
    "      <td>153.14%</td>\n",
    "      <td>4,261</td>\n",
    "      <td>2</td>\n",
    "      <td>35</td>\n",
    "      <td>6,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>15</th>\n",
    "      <td>tennis</td>\n",
    "      <td>16</td>\n",
    "      <td>177.78%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>2,437</td>\n",
    "      <td>4</td>\n",
    "      <td>30</td>\n",
    "      <td>5,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>16</th>\n",
    "      <td>video pinball</td>\n",
    "      <td>496,681</td>\n",
    "      <td>64.64%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>1,083</td>\n",
    "      <td>2</td>\n",
    "      <td>30</td>\n",
    "      <td>5,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>17</th>\n",
    "      <td>wizard of wor</td>\n",
    "      <td>93,090</td>\n",
    "      <td>785.44%</td>\n",
    "      <td>NaN</td>\n",
    "      <td>2,229</td>\n",
    "      <td>4</td>\n",
    "      <td>35</td>\n",
    "      <td>5,000</td>\n",
    "      <td>nan</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the parameter choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent relies on four parmeters:\n",
    "\n",
    "- **Fixed steps**: Although this parameter actually depends on the Environment, we can use it to manually set the frecuancy at which the Agent will play. Taking more consecutive actions allows for exploring further in the future at the cost of less reaction time.\n",
    "\n",
    "\n",
    "- **Time Horizon**: This value represents \"how far we need to look into the future when taking an action\". A useful rule of thumb is **Time Horiozon = Nt / Fixed steps**, where **Nt** is the number of frames that it takes the agent to loose one life, (die) since the moment it performs the actions that inevitably leads to its death. \n",
    "\n",
    "\n",
    "- **Max states**: This is the maximum number of states that can be part of the Swarm. This number is related to \"how thick\" we want the resulting causal cone to be. The algorithm will try to use the maximum number of states possible unless it detects it is wasting computation.\n",
    "\n",
    "\n",
    "- **Max samples**: This is the maximum number of times that we can sample an action when using a Swarm to build a causal cone. It is a superior bound, and the algorithm will try to use less samples to meet the defined **time horizon**. It is a nice way to limit how fast you need to take an action. A reasonable value could be **max walkers** \\* **time horizon** \\* ***N***, being ***N=5*** a number that works well in Atari games, but it depends on what you are trying to accomplish.\n",
    "\n",
    "- **Mean samples**: You cannot directly set this parameter. It is the mean number of samples taken each state. In and ideal case it would be **Max states \\* Time horion** samples. If its lower it means that we are not having any trouble sampling the state space, but if its higher it means that the states tend to die and more computation has been be required. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Minimal Pacman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will tune the Agent to get a decent score on MsPacman using the minimum amount of computational resources possible. We will deliberately set a very small amount of computational reources for calculating an action.\n",
    "\n",
    "Doing we want to address concerns about scalability by showing how the algorithm performs when the area of the causal cone calculated with the Swarm is very little with respect to the size of the state space.\n",
    "\n",
    "In order to do so, we can give the parameters the following values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"MsPacman-v0\"\n",
    "render=True # It is funnier if the game is displayed on the screen\n",
    "clone_seeds = True # This will speed things up a bit\n",
    "max_steps = 1e6 # Play until the game is finished.\n",
    "skip_frames = 80 # The Agent cannot fo anything anyway, so its faster if we skip some frames at the begining\n",
    "n_fixed_steps=5 # taking 4 actions per seconds is more than enough to finish the first level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAI parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples=300 # Let see how well it can perform using 300 samples per step\n",
    "max_states=15 # Let's set a really small number to make everthing faster\n",
    "time_horizon=10 # 50 frames should be enough to realise you have been eaten by a ghost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these parameters we are aiming for 150 samples per step, sving up to another 150 samples in case the agent runs into trouble. Using such a low number of samples will mean that the performance could vary widely among different runs, but in our tests, this agent was capable of finishing the first level most of the times.\n",
    "\n",
    "If you want to get better scores, just increase the values of the parameters accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Minimal Boxing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the same principles as in the former example, but this time aiming to solve the game Boxing. This probably turns out to be the least complex Atari game, and it can be solve with very little amount of computation involved.\n",
    "\n",
    "A funny thing about this game is the only one with the same reward scheme that is supposed to be solved in the minimum amount of time possible. But without stating it explicitely.\n",
    "\n",
    "A fuunny consequence of that, the agent sometimes just starts avoiding the opponent after achieving 99 points, because it considers a terminal state as a death, and it tries to keep himself alive until the time runs out.\n",
    "\n",
    "There are many ways of addressing this problem, but they are out of the scope of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Boxing-v0\"\n",
    "render=True # It is funnier if the game is displayed on the screen\n",
    "clone_seeds = True # This will speed things up a bit\n",
    "max_steps = 1e6 # Play until the game is finished.\n",
    "skip_frames = 5 # Lets start taking actions quickly\n",
    "n_fixed_steps=3 # We want our agent to have fast reflexes. \n",
    "# If you increase the number of samples and choose a fixed step of 1\n",
    "# it becomes Bruce Lee, but taking 10 actions per second is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAI parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples=75 # We like risk! =P\n",
    "max_states=15 # Let's set a really small number to make everthing faster\n",
    "time_horizon=5 # It loses score pretty fast after being hit, so 12 frames should be enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env =  AtariEnvironment(name=name, clone_seeds=clone_seeds)\n",
    "model = RandomDiscreteModel(env.env.action_space.n)\n",
    "greedy = GreedyPolicy(env=env, model=model)\n",
    "\n",
    "fractal = FractalAI(policy=greedy, max_samples=max_samples, max_states=max_states,\n",
    "                    time_horizon=time_horizon, n_fixed_steps=n_fixed_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs faster if you are not recording, uncomment if you want to run a simulation.\n",
    "#fractal.evaluate(render=render, max_steps=max_steps, skip_frames=skip_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record a video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will make everything a bit slower, but you will be recording the game with an OpenAI monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"videos\" # Folder where you can save the video.\n",
    "force = True # Override previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 193 Score: 29.0\n",
      " Fractal with 15 states.\n",
      "Balance Coef 1.1467 | Mean Samples 75.47 | Current sample limit 60\n",
      "Algorithm last iter:\n",
      "- Deaths: \n",
      "    0 states: 0.00%\n",
      "- Time: \n",
      "    Mean: 5.73 | Std: 0.77\n",
      "- Worker: \n",
      "    Step: 93.33% 14 steps | Clone: 6.67% 1 clones\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monitor = AtariMonitorPolicy(policy=fractal, directory=directory, force=force)\n",
    "monitor.record_video(skip_frames=skip_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Check the video folder to watch what you recorded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
